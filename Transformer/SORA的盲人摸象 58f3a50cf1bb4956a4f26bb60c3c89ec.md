# SORA的盲人摸象

从目前的消息来看，SORA是一个先进的视频生成模型，它通过变分自编码器（VAE）将视频编码为隐空间中的时空patches，然后利用基于Transformer的扩散模型对这些patches进行操作，以生成高质量的长视频内容。

[Video generation models as world simulators](https://openai.com/research/video-generation-models-as-world-simulators)

---
[toc]

## SORA是什么？

SORA是一个先进的视频生成模型，它通过变分自编码器（VAE）将视频编码为隐空间中的时空patches，然后利用基于Transformer的扩散模型对这些patches进行操作，以生成高质量的长视频内容。

> We explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.

## 视频是什么？视频是连续的三维场

我可以将一段连续的视频视为一个多维的连续场景，它不仅沿着横向和纵向坐标展开，还在时间轴上延伸。这样，当我们在这三个维度上将其展开时，便可以得到下图所示的形态。这正是SORA处理视频数据的独特之处所在：通过将视频内容编码为沿时间、横向和纵向坐标展开的时空patches，SORA能够在这个高维隐空间中高效地进行视频内容的学习和生成，进而创造出丰富且连贯的视觉叙事。

![field-front-view.jpg](SORA%E7%9A%84%E7%9B%B2%E4%BA%BA%E6%91%B8%E8%B1%A1%2058f3a50cf1bb4956a4f26bb60c3c89ec/field-front-view.jpg)

![field-side-view.jpg](SORA%E7%9A%84%E7%9B%B2%E4%BA%BA%E6%91%B8%E8%B1%A1%2058f3a50cf1bb4956a4f26bb60c3c89ec/field-side-view.jpg)

![field-top-view.jpg](SORA%E7%9A%84%E7%9B%B2%E4%BA%BA%E6%91%B8%E8%B1%A1%2058f3a50cf1bb4956a4f26bb60c3c89ec/field-top-view.jpg)

![field-free-view.jpg](SORA%E7%9A%84%E7%9B%B2%E4%BA%BA%E6%91%B8%E8%B1%A1%2058f3a50cf1bb4956a4f26bb60c3c89ec/field-free-view.jpg)

![Frame-histogram.jpg](SORA%E7%9A%84%E7%9B%B2%E4%BA%BA%E6%91%B8%E8%B1%A1%2058f3a50cf1bb4956a4f26bb60c3c89ec/Frame-histogram.jpg)

## SORA如何使用transformer进行diffusion

SORA与大型语言模型（LLM）的关系体现在其使用transformer结构来处理视频内容的方式上，类似于LLM处理文本数据。就像LLM将文本分解为tokens并使用transformer来理解和生成文本一样。

SORA将视频内容分解为时空patches，并利用transformer的强大能力来进行diffusion过程，从而生成视频。这种方法不仅允许SORA捕捉和生成复杂的视频序列，也意味着视频生成领域可以借鉴和应用LLM在文本处理方面的先进技术和经验，如自注意力机制、序列到序列的建模，以及条件生成等，进而推动视频内容创作的新范式。

SORA如何使用transformer进行diffusion的过程可以概括为以下几个关键步骤：

1. **初始化隐空间**：首先，通过使用VAE将原始视频数据编码到一个高度压缩的隐空间中，形成一系列的patches（砖块）。这些patches可以被视为变换器（transformer）的输入tokens。
2. **噪声添加**：在训练阶段，SORA模型将这些隐空间中的patches作为基础，进而在这些patches上添加噪声。这一步模仿了diffusion过程的前向过程，即逐渐将数据引入一个无序的状态。
3. **基于transformer的预测**：接下来，利用transformer模型的能力，SORA被训练去预测如何从这些带有噪声的patches恢复到它们的原始“干净”状态。这一步骤对应于diffusion过程的反向过程，即从噪声状态逐步恢复到原始清晰状态。
4. **迭代细化**：通过transformer模型，SORA利用其深层的注意力机制来分析和处理隐空间中的patches，从而实现对视频内容的深度理解和生成。在这个过程中，模型会迭代地应用diffusion过程，逐步减少噪声并细化patches的预测。
5. **条件生成**：SORA可以接受条件信息，如文本提示，来指导视频生成过程。通过在transformer模型中融合这些条件信息，SORA能够根据给定的指令生成特定内容的视频。
6. **解码视频**：最后，使用与编码相对应的解码器将隐空间中“清洁”的patches转换回像素空间，从而生成最终的视频结果。

通过上述步骤，SORA利用transformer结构实现了在隐空间中对视频patches进行精细且条件化的diffusion处理，允许高度灵活和创造性的视频内容生成。

## 计算对象：隐空间中的patches砖块

> Whereas LLMs have text tokens, Sora has visual *patches*. Patches have previously been shown to be an effective representation for models of visual data.

在视觉数据处理中，"patches"指的是将图像或视频帧分割成的小块区域。每个patch包含了原始图像的一部分信息，可以是像素的子集。通过对这些小块区域的处理和分析，模型能够学习到图像或视频的局部特征，进而理解整体内容。

在深度学习和机器学习领域，特别是在处理视觉数据时，"patches"（或称为"砖块"）是一种重要的数据表示方法。这种表示方法在SORA这样的模型中被用来处理和生成视频内容，其基本思想与语言模型（LLMs）使用文本tokens作为基本计算单位的方式类似，但是针对的是视觉信息。

![Untitled](SORA%E7%9A%84%E7%9B%B2%E4%BA%BA%E6%91%B8%E8%B1%A1%2058f3a50cf1bb4956a4f26bb60c3c89ec/Untitled.png)

## 步骤一：VAE，建立视频与隐空间的映射

在SORA模型中，patches不仅仅是对视觉数据的一种表示方法，它们还被编码到一个高维的隐空间中，这个过程通常通过变分自编码器（VAE）完成。隐空间中的patches因此包含了原始视觉数据的压缩表示，这样的表示捕获了视频的关键信息，同时去除了冗余，使得数据处理变得更加高效。

### VAE如何“编码”隐空间

> We train a network that reduces the dimensionality of visual data. This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially.

VAE是一种深度学习模型，它通过学习输入数据的高效表示（即隐表示）来实现数据的压缩和生成。在SORA模型中，VAE用于将原始视频数据编码到一个隐空间中，这个过程涉及以下关键步骤：

1. **输入处理**：VAE首先接收原始视频数据作为输入。这些视频数据被分割成patches，与语言模型处理文本tokens的方式类似，这些patches作为模型的基本处理单位。
2. **编码器网络**：接下来，视频数据通过VAE的编码器网络。这个网络是一个深度神经网络，它逐步降低数据的维度，提取视频内容的关键特征，并将这些特征表示为隐空间中的点。这个过程涉及到从原始高维数据中学习到一个低维且有意义的表示，该表示捕获了视频数据的核心信息。
3. **隐表示**：编码器的输出是一个或多个隐向量，这些向量构成了视频数据在隐空间中的表示。这个隐空间是高度压缩的，可以看作是原始视频数据的一个紧凑摘要。隐向量包含了重建原始数据所需的所有关键信息，但占用的存储空间远小于原始视频。
4. **随机性引入**：VAE通过在编码过程中引入随机性（例如，通过向隐表示添加噪声）来增强模型的泛化能力和生成能力。这种随机性使得从隐空间中采样得到的向量能够被解码成多样化的输出，增加了生成内容的多样性和创新性。

### 隐空间如何“解码”得到视频

> We also train a corresponding decoder model that maps generated latents back to pixel space.

1. **解码器网络**：从隐空间中得到的隐向量接着被送入VAE的解码器网络。解码器的任务是执行编码过程的逆过程，即将隐向量映射回原始数据空间。通过逐步增加数据的维度，解码器重建出与原始视频数据相似的内容。
2. **视频重建**：解码器输出的是重建的视频数据的patches。这些patches在像素层面上被重新组合，形成完整的视频帧。通过精细地调整解码器的参数，模型能够生成与原始视频非常相似的视觉内容。
3. **质量和细节的恢复**：虽然隐空间表示是高度压缩的，解码过程能够通过学习到的特征和模式来重建视频的细节和质量。这表明，即使在显著降低数据维度的情况下，VAE也能够保留视频内容的关键属性。

通过这种编码-解码机制，VAE在SORA模型中实现了视频数据与隐空间之间的有效映射，为后续的视频生成和编辑提供了强大的基础。

## 步骤二：SORA在隐空间中训练和生成

### 隐空间中的patches保持视频的时空结构

> Sora is trained on and subsequently generates videos within this compressed latent space. (Spacetime latent patches)

SORA模型通过在一个压缩的隐空间内训练和生成视频，有效利用了所谓的“时空latent patches”。这些patches不仅包含了视频的空间信息（即图像的局部区域），还包含了时间信息（即随时间变化的图像序列）。这种表示方式允许SORA捕捉和保持视频内容的时空连贯性，即如何随时间演变。

- **时空连贯性**：通过处理这些时空patches，SORA能够理解并生成具有时间连续性的视频序列，例如物体的运动和场景的变换。
- **高效处理**：在隐空间中处理视频数据还意味着更高效的计算，因为与原始视频相比，数据的维度和复杂度都被大幅度降低。

### 隐空间中的patches等价于LLM中的token

> Given a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens.
> 

在SORA中，隐空间的patches在功能上等同于语言模型（LLM）中的tokens。这些patches作为Transformer模型的输入单位，使得SORA能够以一种顺序和结构化的方式处理视频数据。

- **序列处理**：就像LLM处理一串文本tokens以生成文本内容一样，SORA通过处理一系列时空patches来生成视频内容。
- **多样性和创造性**：这种方法允许SORA模型捕捉到丰富的视频内容特性，并在生成阶段创造出新颖的视频序列。

### 在隐空间中（操作patches）实现图像变换

这种在隐空间中操作patches的方法，结合了VAE的压缩能力和Transformer模型的强大序列处理能力，使SORA能够高效且创造性地处理和生成视频内容，开启了视频生成领域的新可能。

- **训练阶段**：在训练阶段，SORA通过处理不同分辨率、时长和纵横比的视频和图像上的patches来学习视频内容的底层结构和动态。这种灵活性使得模型能够从多样化的数据中学习，提高了其泛化能力和对新视频内容的适应能力。
- **生成阶段**：在生成或推理阶段，通过在适当大小的网格中排列随机初始化的patches，SORA可以控制生成视频的大小。这意味着模型能够根据需要生成不同分辨率和纵横比的视频，展示了其对视频生成过程的高度控制能力。

> Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. ……At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.

## 步骤三：SORA通过transformer结构实现patches计算，计算的方式是diffusion

> Sora is a diffusion model.

SORA结合了扩散模型（diffusion model）的强大能力和Transformer的高效计算框架，以在隐空间中对视频内容进行建模和生成。这种结合利用了两种方法的优点：扩散模型在生成高质量图像方面的能力，以及Transformer在处理序列数据方面的高效性。

- **扩散过程**：SORA模型通过逐步去除输入patches中的噪声，并在此过程中引入条件信息（如文本提示），来生成目标视频内容。这个去噪声的过程模仿了物理世界中的扩散过程，由此得名。

### 什么是diffusion

diffusion是扩散模型。

> Sora is a diffusion model; given input noisy patches (and conditioning information like text prompts), it’s trained to predict the original “clean” patches.

扩散模型（diffusion model）是一种生成模型，它通过模拟从有序状态到无序状态的扩散过程，然后再逆向这个过程来生成数据。在视频生成的上下文中，这意味着模型首先将原始视频patches逐步加入噪声，直到它们变得完全随机无序；然后，模型学习如何逆转这个过程，从噪声状态重建出清晰、有意义的视频内容。

- **去噪声训练**：在训练期间，SORA模型学习如何预测在特定噪声水平下原始“清洁”patches的正确版本。这一过程涉及到对大量带有噪声的数据进行处理，模型通过这种方式学习如何恢复视频的原始内容。

![Untitled](SORA%E7%9A%84%E7%9B%B2%E4%BA%BA%E6%91%B8%E8%B1%A1%2058f3a50cf1bb4956a4f26bb60c3c89ec/Untitled%201.png)

### 什么是transformer

transformer是目前LLM中最好用的基础结构。

> Importantly, Sora is a diffusion *transformer*.

Transformer是一种基于自注意力机制的深度学习模型，它能够处理序列数据，如文本或时间序列数据，同时考虑序列内各元素之间的全局依赖关系。在SORA中，Transformer的架构被用来处理隐空间中的patches，这允许模型有效地捕捉视频内容的复杂时空关系。

- **自注意力机制**：自注意力机制使Transformer能够关注输入序列中的不同部分，对于视频内容生成来说，这意味着模型可以同时考虑视频中的空间位置和时间进程，从而更好地理解和生成动态场景。

### SORA如何使用transformer进行diffusion

SORA通过将diffusion过程嵌入到Transformer的结构中来实现高效的视频内容生成。模型在每一步都使用Transformer的自注意力机制来处理带噪声的patches，同时考虑它们之间的空间和时间关系。通过这种方式，SORA能够逐步去除噪声，同时保持视频内容的连贯性和逻辑性。

- **条件生成**：SORA可以接受条件信息（如文本提示），这些信息通过Transformer模型传递，引导去噪过程以生成与条件相匹配的视频内容。这种方法使得生成的视频不仅质量高，而且与给定的文本描述紧密相关，为创造性视频内容的生成开辟了新路径。
- **与LLM的关系**：SORA与LLM的关系体现在其使用transformer结构来处理视频内容的方式上，类似于LLM处理文本数据。就像LLM将文本分解为tokens并使用transformer来理解和生成文本一样，SORA将视频内容分解为时空patches，并利用transformer的强大能力来进行diffusion过程，从而生成视频。这种方法不仅允许SORA捕捉和生成复杂的视频序列，也意味着视频生成领域可以借鉴和应用LLM在文本处理方面的先进技术和经验，如自注意力机制、序列到序列的建模，以及条件生成等，进而推动视频内容创作的新范式。