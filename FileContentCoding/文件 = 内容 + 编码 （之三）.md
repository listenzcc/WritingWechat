## 文件 = 内容 + 编码 （之三）

本部分接上文《文件 = 内容 + 编码 （之二）》。

> 我们重新审视编码纠正的工作，会发现 PS 操作过程中，至少暗含着两个未经考察的问题，这两个问题都是针对字节解码的问题，它们分别对应着在纠正代码中所使用的`utf-16`和`latin1`编码协议。
> ……如果我们轻易放过这两个编码问题，我们将错过 W 系统进行中文编码的重要细节，给之后的代码工作留下后患。
> 因此，在接下来的一篇文章，我将对此进行分析。

本文将跳出图像矩阵编码的范畴，讨论在计算机系统中较为一般的符号编码问题。
但是由于该问题过于繁杂，本文将集中讨论 Python 语言环境下的二进制编码问题，以及中文符号加入后，给原先的拉丁字母表达系统造成的麻烦。
另外，由于本部分内容可能包含过多的细节，为了避免失于细碎，我们不妨先将其中容易出现理解偏差的关键节点列写出来，再逐一作出解析。

---

## 一般二进制序列的编码

### 童话式的自洽逻辑及其陷阱

上一节中的图像传输过程，看上去是经过了由二进制编码到数字，再回到二进制编码的处理路线。
然而，如果这样理解，就无疑会忽略至少两个关键的技术节点，以及数字的存储和读取方式，总共三个陷阱。
我们首先把这三个陷阱在处理流程上标记出来，再做逐一介绍。

具体来说，系统获取二进制编码，对二进制编码进行切分，形成一列数字（**技术节点一**），这一列数字写入文件，再从文件中读取这些数字（**数字存储和读取**），从数字还原出最初的二进制编码（**技术节点二**）。

### 技术节点一

在这一节点中，二进制序列并没有经过任何意义上的编码。
而是单纯的二进制数字到十进制数字之间的转换。
比如形如下式的对应关系。

```
(DEC) 137 = (BIN) 1000 1001 = (HEX) 89
```

由于切分的长度为`8`比特，我们可以使用这样的规则，将二进制序列转换为`0 - 255`之间的正整数序列。
目前为止还十分简洁，但问题在于，如何将正整数列还原为原始的二进制数列，即技术节点二。

### 技术节点二

该节点的目的是在于如何将正整数还原为 `8` 位的二进制数。
遗憾的是，目前计算机系统中所使用的编码方式百花齐放，这导致了反向的解码结果并非是唯一的。
比如，

- ASCII 编码是`7`比特的字符集，涵盖了英语中的绝大多数字符，编码从`0`到`127`（这与本例的 `8` 比特编码并不兼容）；
- ISO Latin-1 是`8`比特的字符集，定义了`256`个字符。前`128`个字符(`00000000-01111111`)与 ASCII 完全一致（这也是本例中所使用的编码方式）；
- UTF-8 采用可变长度的编码，长度从`1`到`4`个字节，即 `8` 到 `32` 个比特位不等（这是十分容易出现问题的编码）；
- 由于除 ASCII 编码之外，几乎所有的编码都用满了一个字节，即 `8` 位二进制数的长度，因此都或多或少地利用到添加前导字节的技术，来标记特殊字符，用来避免潜在错误，这往往就是各种莫名其妙编码问题的由来。

在本例中，若使用 utf-8 编码对数字进行解码，我们会在值大于 `127` 时，在解码结果中获得形如 `\xc2` 的前导字节，这无疑会导致后续的解码工作。
当然这只是一个极其特殊的例子，但是它给我们标示出了一个潜在的问题，即在采用了不合适的编码规则的情况下，程序可能不会立即显示出不正常的行为，但极其可能会有不经意的 BUG 存储在系统中，十分需要程序员注意。

### 数字存储和读取

最后，我们来到一个似是而非的问题，即计算机真的能够把数字存储到文件中吗？
这样的文件真的能够以文本的方式进行打开吗？
最直观的答案是肯定的，但事实是残酷的否定。

> 因为你可以思考这样一种情况，对于数字 `137` 来说，我们如果在某个文本文件中看到它，我们当然可以用鼠标选中其中的 `1` 、`3` 或者 `7` 中的任意一个数字，而逻辑上讲，`137` 是一个整体，如果计算机真的把它存储在了文件中，我们是无法单独获取其中的任意一个位值而不碰其他的值的。
> （这涉及一个哲学问题，这就是，整体的部分的集合还是整体本身吗？
> 放心，这里只提问，而不做任何形式的讨论和解答）。

那么，计算机存储下来的数字是什么呢？
其实它们不再是数字，而是形如数字的字符串。
比如`137` 不是一个数字，而是`'1' + '3' + '7' + '\0'`这样的一个字符串，最后的`\0`代表文件数据中的字符串的终止标记。

综上所述，我们在文件中所面对的一列“数字”，在实质上是一列“字符”的组合。
而 W 系统使用`utf-16`编码对字符进行存储。所以我们当然需要使用该编码来解析它。

### 扩展编码集所导致的问题

然而，utf-16 编码方式的使用也是一个值得思考的问题。
事实上，就单纯的数字和拉丁字母而言，因为它们的数量有限，我们无须使用如此长的二进制编码。
但 W 系统提供了支持中文的语言包，由于中文字符数量远超拉丁字母，这就需要在编码端使用更长、更复杂的编码方案，以便扩充它的表达集合。
目前，最主流的兼容包括中文在内的多种语言字符的解决方案，是称为 Unicode 的字符集。
为了比 utf-8 更加完整地覆盖 Unicode 字符集（Unicode 的范围为`0x0~0x10FFFF`，可以用来表示大量的特异性字符），计算机系统必须做出广度和效率之间的妥协。

但这样做也导致了一个附加的问题，这就是用户必须也在广度和效率之间做出选择。
说人话，就是对于目前计算机系统中的任何一个文件，它都处于一个“薛定谔的猫”的状态，既有可能是使用效率较高的 utf-8 编码方案，也有可能是使用广度较高的 utf-16 或甚至更加复杂编码方案。
吊诡的是，单从文件中所包含的二进制序列来看，用户甚至无法单从内容判断出它到底属于哪种文件。
这是几乎一切编码问题的由来。

对此，我也无法提出任何良好方案。
只能提醒读者，在处理二进制文件的过程中，一定要事先确定它是如何进行编码的，这样可以有效避免程序的不确定行为，切记切记。
