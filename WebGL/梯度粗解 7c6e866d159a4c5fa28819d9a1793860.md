# 梯度粗解

梯度就是局部效用最大的变化方向。对于给定的标量场，我们总能让某个点沿着它的梯度进行“移动”。移动的过程中，该点的值会不断增加。因此这个过程称为梯度上升。如果将这个场倒过来，那么不断增大的过程会转换成不断减小的过程，这个过程就是“梯度下降”。

开源代码可见我的 ObservableHQ 笔记本

[Searching maximum by gradient ascending](https://observablehq.com/@listenzcc/searching-maximum-by-gradient-ascending)

---
- [梯度粗解](#梯度粗解)
  - [二维标量场的梯度](#二维标量场的梯度)
  - [梯度下降及其稳定性](#梯度下降及其稳定性)


## 二维标量场的梯度

标量场的定义是二元函数

$$
f(x, y) \in \mathbb{R}
$$

方向导数是其偏导数

$$
f_x :=\frac{\partial f}{\partial x} = \lim_{dx\rightarrow 0}\frac{f(x+dx, y)-f(x, y)}{dx}
$$

另一个方向的偏导数 $f_y$ 同理。考虑新向量

$$
d = [f_x, f_y]
$$

将其进行归一化，得到新向量

$$
n=\frac{d}{\vert d \vert}
$$

考虑任意单位向量

$$
m=[d'x, d'y]
$$

注意到下式

$$
n = \argmax_{m} m \cdot n
$$

可知，当坐标位置变化量在两个方向上的比例与 $d$ 向量相同时，函数值的变化量最大，因此人们就将这个新向量规定的方向定义为梯度

$$
grad = i\cdot f_x + j \cdot f_y
$$

简单来说，梯度就是局部效用最大的变化方向。

## 梯度下降及其稳定性

对于给定的标量场，我们总能让某个点沿着它的梯度进行“移动”。移动的过程中，该点的值会不断增加。因此这个过程称为梯度上升。如果将这个场倒过来，那么不断增大的过程会转换成不断减小的过程，这个过程就是“梯度下降”。

如下动图展示了从 4 个不同的点开始进行梯度下降的迭代轨迹。从结果中可以看到，由于迭代的起点各异，因此当全局最大（小）值不存在时，迭代的终点可能有较大的差异。当全局极值存在时，这些起始点会收敛到相同的极值点，当然，这是最理想的情况。

值得一提的是，我在以上段落中未加解释地混用了一些概念，在此简要解释如下：

- 迭代与收敛，在机器学习场景中，迭代是模型寻优的重要手段，迭代的目标是收敛；
- 极值与最值，这是两个不同的数学概念，极值往往代表局部的梯度为零的点，而最值代表整个定义域内最大（小）的点。

在机器学习场景中，我们希望待解决的问题具有以下属性：

- 有单一极值的；
- 单一极值就是最值的；
- 迭代能够收敛的

遗憾的是，这类问题已经所剩无几了。

![20231110-162020.gif](%E6%A2%AF%E5%BA%A6%E7%B2%97%E8%A7%A3%207c6e866d159a4c5fa28819d9a1793860/20231110-162020.gif)

![Untitled](%E6%A2%AF%E5%BA%A6%E7%B2%97%E8%A7%A3%207c6e866d159a4c5fa28819d9a1793860/Untitled.png)

![Untitled](%E6%A2%AF%E5%BA%A6%E7%B2%97%E8%A7%A3%207c6e866d159a4c5fa28819d9a1793860/Untitled%201.png)

![Untitled](%E6%A2%AF%E5%BA%A6%E7%B2%97%E8%A7%A3%207c6e866d159a4c5fa28819d9a1793860/Untitled%202.png)