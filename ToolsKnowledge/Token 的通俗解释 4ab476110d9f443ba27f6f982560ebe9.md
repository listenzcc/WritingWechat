# LLM ä¸­ Token çš„é€šä¿—è§£é‡Š

æœ¬æ–‡ä½¿ç”¨ python çš„ transformers åŒ…æä¾›çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œ token è§£æï¼Œå¹¶å°è¯•é€šè¿‡è§£æç»“æœæ¥å›ç­” token æ˜¯ä»€ä¹ˆçš„é—®é¢˜ã€‚é€šè¿‡å‡ ä¸ªä¾‹å­çœ‹åˆ°ï¼Œåœ¨ä¸åŒçš„è¯­å¢ƒä¸‹ï¼Œç›¸åŒçš„ token ç»è¿‡è¯­è¨€æ¨¡å‹è®¡ç®—ä¹‹åï¼Œå¯ä»¥å¾—åˆ°ä¸åŒçš„ç‰¹å¾å‘é‡ã€‚è¿™è¯´æ˜ LLM åœ¨ token çš„ç‰¹å¾å‘é‡è¿™ä¸€å±‚çº§å·²ç»å¼€å§‹å¯¹è¯­ä¹‰ä¿¡æ¯è¿›è¡Œå¤„ç†ï¼Œå¤„ç†çš„åŸºç¡€æ˜¯ token å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚

æœ¬æ–‡å¼€æºä»£ç å¯è§æˆ‘çš„ Github ä»“åº“

[https://github.com/listenzcc/learn-tokenizer](https://github.com/listenzcc/learn-tokenizer)

---
- [LLM ä¸­ Token çš„é€šä¿—è§£é‡Š](#llm-ä¸­-token-çš„é€šä¿—è§£é‡Š)
  - [LLM çš„ token](#llm-çš„-token)
  - [Token ä¸å‘é‡](#token-ä¸å‘é‡)
  - [Token å‘é‡çš„é€šä¿—è§£é‡Š](#token-å‘é‡çš„é€šä¿—è§£é‡Š)
  - [ä¸­è‹±æ–‡ token ç‰¹å¾çš„å¯¹æ¯”](#ä¸­è‹±æ–‡-token-ç‰¹å¾çš„å¯¹æ¯”)
  - [é™„å½•ï¼šToken è®¡ç®—ä»£ç ](#é™„å½•token-è®¡ç®—ä»£ç )


## LLM çš„ token

Token åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æœ‰ç‰¹æ®Šçš„æ„ä¹‰ï¼Œæ˜¯ LLM è¿›è¡Œå¤„ç†çš„æœ€å°å•å…ƒã€‚ChatGPT æ¨¡å‹å¯¹ token çš„è§£é‡Šæ˜¯

> The GPT family of models process text usingÂ **tokens**, which are common sequences of characters found in text. The models understand the statistical relationships between these tokens, and excel at producing the next token in a sequence of tokens.
> 

å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸º token çš„è¿‡ç¨‹ç§°ä¸º tokenization

> Tokenization is the process of splitting the input and output texts into smaller units that can be processed by the LLM AI models. Tokens can be words, characters, subwords, or symbols, depending on the type and the size of the model.
> 

BERT æ¨¡å‹å¯¹ token çš„ä½¿ç”¨æ–¹æ³•æ˜¯

> BERT uses tokens as the basic input units to process text. These tokens are generated through the process of tokenization, which involves breaking down the input text into smaller subword units or characters.
> 

## Token ä¸å‘é‡

æœ¬æ–‡ä½¿ç”¨ python çš„ transformers åŒ…æä¾›çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œ token è§£æï¼Œå¹¶å°è¯•é€šè¿‡è§£æç»“æœæ¥å›ç­” token æ˜¯ä»€ä¹ˆçš„é—®é¢˜ï¼Œæ¨¡å‹éƒ¨ç½²çš„æ ¸å¿ƒä»£ç å¦‚é™„å½•æ‰€ç¤ºã€‚ä»¥ä¸€æ®µè‹±æ–‡ä¸ºä¾‹

> Many words map to one token, but some don't: indivisible.
> 
> 
> Unicode characters like emojis may be split into many tokens containing the underlying bytes: ğŸ¤šğŸ¾
> 
> Sequences of characters commonly found next to each other may be grouped together: 1234567890
> 

å®ƒçš„ token æ—¢ä¸æ˜¯ä»¥å•è¯ä¸ºå•ä½ï¼Œä¹Ÿä¸æ˜¯ä»¥å•è¯çš„ç»„åˆä¸ºå•ä½ï¼Œè€Œæ˜¯ä»¥å•è¯çš„è¯æ ¹ä¸ºå•ä½ï¼Œè¢«åˆ‡åˆ†ä¸ºæ¨¡å‹èƒ½å¤Ÿè®¤è¯†çš„ tokenï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å›¾ä¸­æ¯ä¸€ä¸ªç‚¹éƒ½ä»£è¡¨ä¸€ä¸ª tokenï¼Œå®ƒä»¬æ—¢æœ‰å®Œæ•´çš„å•è¯ï¼Œåˆæœ‰äº•å·å¼€å¤´çš„è¯æ ¹ï¼Œè¿˜æœ‰[CLS]ã€[UNK]ã€[SEP]ç­‰ä»£è¡¨çš„ç‰¹æ®Šå­—ç¬¦ã€‚

![Untitled](Token%20%E7%9A%84%E9%80%9A%E4%BF%97%E8%A7%A3%E9%87%8A%204ab476110d9f443ba27f6f982560ebe9/Untitled.png)

æˆ‘ä»¬ä»æ¨¡å‹å¾—åˆ°çš„ç‰¹å¾å¹¶ä¸æ˜¯å›¾ä¸­æ¨ªçºµåæ ‡è¡¨ç¤ºçš„äºŒç»´å‘é‡ï¼ˆfeature-1 å’Œ feature-2ï¼‰ï¼Œè€Œæ˜¯ 1024 ç»´çš„å‘é‡ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚æˆ‘æ˜¯é€šè¿‡é™ç»´ç®—æ³• Isomap å°† 1024 ç»´çš„å‘é‡é™ç»´åˆ° 2 ç»´ç©ºé—´ä¸­å¯¹å®ƒä»¬è¿›è¡Œå±•ç¤ºï¼Œå½¢æˆä¸Šå›¾ã€‚

![Untitled](Token%20%E7%9A%84%E9%80%9A%E4%BF%97%E8%A7%A3%E9%87%8A%204ab476110d9f443ba27f6f982560ebe9/Untitled%201.png)

## Token å‘é‡çš„é€šä¿—è§£é‡Š

ä¸ºäº†è¿›ä¸€æ­¥è§£é‡Šè¯´æ˜ token æ˜¯ä»€ä¹ˆï¼Œæˆ‘ä½¿ç”¨å¦å¤–ä¸€ä¸ªä¾‹å­ï¼Œé‚£å°±æ˜¯ç”¨ä¸Šè¿°å¤„ç†æ–¹æ³•å¯¹ä»¥ä¸‹è¯­å¥è¿›è¡Œå¤„ç†

- æ±½è½¦çš„ç”Ÿå‘½æ˜¯æ±½æ²¹
- æ±½è½¦çš„æ±½æ²¹æ˜¯ç”Ÿå‘½
- ç”Ÿå‘½çš„æ±½è½¦æ˜¯æ±½æ²¹
- æ±½æ²¹æ˜¯çŸ³æ²¹çš„å‰¯äº§å“
- çŸ³æ²¹æ›¾ç»æ˜¯çŸ³å¤´

å¯¹è¿™ 5 å¥è¯è¿›è¡Œ token åˆ†æåå¾—åˆ°ä¸‹å›¾ï¼Œå…¶ä¸­å·¦å›¾æ˜¯æŒ‰å¥å­é¡ºåºç€è‰²çš„ token ä½ç½®ï¼Œå³å›¾æ˜¯è¿™äº› token çš„ 1024 ç»´å‘é‡ä¹‹é—´çš„äº’ç›¸å…³çŸ©é˜µã€‚

![Untitled](Token%20%E7%9A%84%E9%80%9A%E4%BF%97%E8%A7%A3%E9%87%8A%204ab476110d9f443ba27f6f982560ebe9/Untitled%202.png)

![Untitled](Token%20%E7%9A%84%E9%80%9A%E4%BF%97%E8%A7%A3%E9%87%8A%204ab476110d9f443ba27f6f982560ebe9/Untitled%203.png)

ä¸ºäº†æ¸…æ¥šèµ·è§ï¼Œæˆ‘å°†ä¸Šå›¾ä¸­çš„å·¦å›¾æ”¾å¤§å¦‚ä¸‹ï¼Œå¹¶ä¸”æŒ‰ token å¯¹ç‚¹è¿›è¡Œç€è‰²ï¼Œè¿˜è¿›ä¸€æ­¥å°†è¿™äº›ç‚¹è¿æ¥èµ·æ¥ï¼Œå¾—åˆ°ä¸‹å›¾ã€‚ä»ä¸‹å›¾ä¸­å¯ä»¥çœ‹åˆ°ï¼Œè™½ç„¶æ¯ä¸ªåˆ†å¥çš„ token æœ‰é‡å¤çš„æˆåˆ†ï¼Œä½†ç»è¿‡è¯­è¨€æ¨¡å‹è®¡ç®—ä¹‹åï¼Œç›¸åŒçš„ token å¯¹åº”ä¸åŒçš„ç‰¹å¾å‘é‡ã€‚è¿™è¯´æ˜åœ¨ä¸åŒçš„è¯­å¢ƒä¸‹ï¼Œç›¸åŒçš„ token ç»è¿‡è¯­è¨€æ¨¡å‹è®¡ç®—ä¹‹åï¼Œå¯ä»¥å¾—åˆ°ä¸åŒçš„ç‰¹å¾å‘é‡ã€‚

![Untitled](Token%20%E7%9A%84%E9%80%9A%E4%BF%97%E8%A7%A3%E9%87%8A%204ab476110d9f443ba27f6f982560ebe9/Untitled%204.png)

æ¥ä¸‹æ¥ï¼Œæˆ‘å°†ä¸åŒçš„å¥å­åˆ†å¼€ç»˜å›¾ï¼Œå¹¶ä¸”å°†è¿™äº› token æŒ‰å‡ºç°çš„é¡ºåºè¿æ¥èµ·æ¥ï¼Œå¾—åˆ°ä¸€äº›æ›²çº¿ã€‚å¯ä»¥çœ‹åˆ°ï¼Œæ¯æ¡æ›²çº¿éƒ½æ˜¯ä»¥ [CLS] ä¸ºèµ·ç‚¹ï¼Œä»¥ [SEP] ä¸ºç»ˆç‚¹çš„å¯å˜é•¿åº¦æ‹†çº¿ï¼Œé•¿åº¦ä¸ºè¯¥å¥ä¸­çš„ token æ•°é‡ã€‚

![Untitled](Token%20%E7%9A%84%E9%80%9A%E4%BF%97%E8%A7%A3%E9%87%8A%204ab476110d9f443ba27f6f982560ebe9/Untitled%205.png)

æœ€åï¼Œæˆ‘å¯¹è¿™äº›å‘é‡è¿›è¡Œç®€å•çš„ KMeans èšç±»åˆ†æï¼Œç”¨ç±»åˆ«å¯¹ç‚¹è¿›è¡Œç€è‰²ï¼Œå¯ä»¥çœ‹åˆ°å®ƒä»¬åœ¨ 1024 ç»´çš„ç‰¹å¾ç©ºé—´å‘ˆç°è¯­ä¹‰ç›¸è¿‘çš„åŒ¹é…å…³ç³»ã€‚

![Untitled](Token%20%E7%9A%84%E9%80%9A%E4%BF%97%E8%A7%A3%E9%87%8A%204ab476110d9f443ba27f6f982560ebe9/Untitled%206.png)

![Untitled](Token%20%E7%9A%84%E9%80%9A%E4%BF%97%E8%A7%A3%E9%87%8A%204ab476110d9f443ba27f6f982560ebe9/Untitled%207.png)

## ä¸­è‹±æ–‡ token ç‰¹å¾çš„å¯¹æ¯”

ä¸‹æ–¹çš„äº’ç›¸å…³çŸ©é˜µå›¾ä»£è¡¨ä¸­è‹±æ–‡å¤¹æ‚çš„ä¸¤æ®µè¯çš„ token å‘é‡ä¹‹é—´çš„ç›¸ä¼¼å…³ç³»ï¼Œå¯ä»¥çœ‹åˆ°ä¸­æ–‡ä¸è‹±æ–‡å½¼æ­¤çš„ç›¸ä¼¼ç¨‹åº¦å°äºè¯­è¨€å†…çš„ç›¸ä¼¼ç¨‹åº¦ã€‚

å³å›¾æ˜¯æ˜ å°„åˆ° 2 ç»´ç©ºé—´çš„èšç±»å›¾ï¼Œå›¾ä¸­å¯è§è‹±æ–‡ token é›†ä¸­åœ¨å³ä¸‹è§’ã€æ•°å­— token é›†ä¸­åœ¨å·¦ä¸‹è§’ï¼Œå…¶ä½™ä¸ºä¸­æ–‡ tokenã€‚è¿™è¯´æ˜ LLM åœ¨ token çš„ç‰¹å¾å‘é‡è¿™ä¸€å±‚çº§å·²ç»å¼€å§‹å¯¹è¯­ä¹‰ä¿¡æ¯è¿›è¡Œå¤„ç†ï¼Œå¤„ç†çš„åŸºç¡€æ˜¯ token å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚

![Untitled](Token%20%E7%9A%84%E9%80%9A%E4%BF%97%E8%A7%A3%E9%87%8A%204ab476110d9f443ba27f6f982560ebe9/Untitled%208.png)

![Untitled](Token%20%E7%9A%84%E9%80%9A%E4%BF%97%E8%A7%A3%E9%87%8A%204ab476110d9f443ba27f6f982560ebe9/Untitled%209.png)

## é™„å½•ï¼šToken è®¡ç®—ä»£ç 

```python
'''
Example for tokenization of input text
'''

# Import modules
from transformers import AutoTokenizer, BertModel
from IPython.display import 

# Load pre-trained model ~ hundreds MB size.
tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
model = BertModel.from_pretrained("bert-base-chinese")

# Setup input text
text_input = '<-- Your input text goes here -->'

# Generate model inputs as kwargs,
# it tokenizes the text_input.
inputs = tokenizer(text_input, return_tensors="pt")

# Forward the inputs
outputs = model(**inputs)

# The last hidden state refers the feature of the tokens
features = outputs.last_hidden_state
```