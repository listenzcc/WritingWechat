# 用激活函数“纠正”脑电数据

这是使脑电数据“看上去”正常的一个方法，但它的安全性我还没有考察。

本文的目的是开一个头，要追究的问题是“神经网络是如何看待和处理脑电数据的”，于是首当其冲的就是激活函数对信号的变换原理。本文只阐述现象，原理等我弄明白再补充。

---
- [用激活函数“纠正”脑电数据](#用激活函数纠正脑电数据)
  - [信号与噪声](#信号与噪声)
  - [处理前后对比图](#处理前后对比图)
- [附录：激活函数](#附录激活函数)
  - [ReLU激活函数](#relu激活函数)
  - [Sigmoid激活函数](#sigmoid激活函数)
  - [Tanh激活函数](#tanh激活函数)


## 信号与噪声

首先，当脑电信号如下图所示时，它的质量肯定是无法保证的。对其进行滤波后，在 $\theta$ 和 $\alpha$波段的波形如下图所示，这表明滤波方法对这种噪声没有什么办法。

![Untitled](%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E2%80%9C%E7%BA%A0%E6%AD%A3%E2%80%9D%E8%84%91%E7%94%B5%E6%95%B0%E6%8D%AE%20c497e3d374ec479aa58663f76b0135e1/Untitled.png)

![Untitled](%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E2%80%9C%E7%BA%A0%E6%AD%A3%E2%80%9D%E8%84%91%E7%94%B5%E6%95%B0%E6%8D%AE%20c497e3d374ec479aa58663f76b0135e1/Untitled%201.png)

![Untitled](%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E2%80%9C%E7%BA%A0%E6%AD%A3%E2%80%9D%E8%84%91%E7%94%B5%E6%95%B0%E6%8D%AE%20c497e3d374ec479aa58663f76b0135e1/Untitled%202.png)

在保持信号大小关系不变的前提下，我们可以通过 tanh 激活函数对它进行变换，可以有效降低后续分析的噪声范围。

$$
f(x) = tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

![Untitled](%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E2%80%9C%E7%BA%A0%E6%AD%A3%E2%80%9D%E8%84%91%E7%94%B5%E6%95%B0%E6%8D%AE%20c497e3d374ec479aa58663f76b0135e1/Untitled%203.png)

![Untitled](%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E2%80%9C%E7%BA%A0%E6%AD%A3%E2%80%9D%E8%84%91%E7%94%B5%E6%95%B0%E6%8D%AE%20c497e3d374ec479aa58663f76b0135e1/Untitled%204.png)

![Untitled](%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E2%80%9C%E7%BA%A0%E6%AD%A3%E2%80%9D%E8%84%91%E7%94%B5%E6%95%B0%E6%8D%AE%20c497e3d374ec479aa58663f76b0135e1/Untitled%205.png)

但这种变换不是没有代价的，因为它限制了信号的动态范围，强行将其约束在 $(0, 1)$ 范围内。虽然该方法降低了信号的伪迹，如频谱图中约 5000 秒处的垂直细线；但在时频分析图中，它同时改变了各个波段和各个局部时间内信号能量的变化趋势，如下图蓝色线所示。

从处理前后的结果对比图可以看到，由于在处理前的一些时刻，信号出现了从 $\delta$ 波段延伸到 $\alpha$ 波段的连续“亮线”，因此有理由推测它是由于信号“突变”而造成的伪迹，这些突变称为冲激信号，其引起的响应称为冲激响应。由于冲激信号的频谱通常具有较宽的带宽，因此冲激的宽度越窄，则影响的频谱范围越大。

而通过激活函数对信号进行处理后，这些冲激函数的幅值被减小了，它们的影响自然也削弱了，从图上看，它被限制在了 $\delta$ 波段内。这就解释了蓝线从异常高值，回归到正常范围的原理。也解释我为什么推测未处理的信号处于受“污染”的状态。

幸运的是，即使是在蓝色信号“回归”到正常范围之后，在 700m 处时它还是显示出了被“抑制”的状态，这说明这组噪声数据还是能够捕风捉影一些东西的。

## 处理前后对比图

原始信号：

![Untitled](%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E2%80%9C%E7%BA%A0%E6%AD%A3%E2%80%9D%E8%84%91%E7%94%B5%E6%95%B0%E6%8D%AE%20c497e3d374ec479aa58663f76b0135e1/Untitled%206.png)

![newplot (61).png](%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E2%80%9C%E7%BA%A0%E6%AD%A3%E2%80%9D%E8%84%91%E7%94%B5%E6%95%B0%E6%8D%AE%20c497e3d374ec479aa58663f76b0135e1/newplot_(61).png)

![Untitled](%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E2%80%9C%E7%BA%A0%E6%AD%A3%E2%80%9D%E8%84%91%E7%94%B5%E6%95%B0%E6%8D%AE%20c497e3d374ec479aa58663f76b0135e1/Untitled%207.png)

处理后信号：

![Untitled](%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E2%80%9C%E7%BA%A0%E6%AD%A3%E2%80%9D%E8%84%91%E7%94%B5%E6%95%B0%E6%8D%AE%20c497e3d374ec479aa58663f76b0135e1/Untitled%208.png)

![newplot (62).png](%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E2%80%9C%E7%BA%A0%E6%AD%A3%E2%80%9D%E8%84%91%E7%94%B5%E6%95%B0%E6%8D%AE%20c497e3d374ec479aa58663f76b0135e1/newplot_(62).png)

![Untitled](%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E2%80%9C%E7%BA%A0%E6%AD%A3%E2%80%9D%E8%84%91%E7%94%B5%E6%95%B0%E6%8D%AE%20c497e3d374ec479aa58663f76b0135e1/Untitled%209.png)

---

# 附录：激活函数

附录内容为 AI 补写。

激活函数是神经网络模型中非常重要的一部分，它们可以控制神经元的输出，以便模型有助于提高准确性和性能。本文将讨论激活函数及其在深度学习中的应用。

激活函数是神经网络模型中不可或缺的一部分，它们可以控制神经元的输出，以便模型有助于提高准确性和性能。ReLU，Sigmoid和Tanh激活函数是三种最常用的激活函数，它们有助于解决梯度消失问题，以及防止过拟合和提高模型准确性。

## ReLU激活函数

ReLU（Rectified Linear Unit）激活函数是一种常用的激活函数，它将输入的任何数值转换为正数，以保护神经元的输出。例如，如果输入是-2，则输出将是0。ReLU激活函数用于解决许多神经网络模型中的梯度消失问题。

## Sigmoid激活函数

Sigmoid激活函数是另一种常用的激活函数，它将输入数值转换为0到1之间的实数值。它提供了一个折衷的方法，可以有效地控制神经元的输出，从而有助于防止过拟合和提高准确性。

## Tanh激活函数

Tanh激活函数是另一种常见的激活函数，它将输入的任何数值转换为-1到1之间的实数值。它的行为与Sigmoid激活函数类似，但它更加强大，因为它可以提供更强大的非线性行为，从而更好地控制神经元的输出。