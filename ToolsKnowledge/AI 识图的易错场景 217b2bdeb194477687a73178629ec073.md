# AI 识图的易错场景

AI 是人工智能的简称，由于 ChatGPT 等强力产品的问世，已经有很长时间没人诟病 AI 是“人工智障”了。但事实上，AI 还是极易受到攻击。本文尝试进行一个简单的实验，说明 AI 识图还不是特别可靠。

首先，AI 模型对清晰的、未叠加的图像具有良好的识别能力。其次，叠加图的实验表明 AI 尚没有叠加的概念，叠加图的纹理会产生对抗攻击，导致 AI 得出错误结果。

本文的代码可见我的 GitHub 仓库

[https://github.com/listenzcc/hugging-face-image-player](https://github.com/listenzcc/hugging-face-image-player)

---
- [AI 识图的易错场景](#ai-识图的易错场景)
  - [实验方法](#实验方法)
  - [原图及 AI 识别结果](#原图及-ai-识别结果)
  - [叠加图及AI识别结果（错乱）](#叠加图及ai识别结果错乱)
  - [叠加图及AI识别结果（正常）](#叠加图及ai识别结果正常)
  - [附录：AI模型的对抗性攻击](#附录ai模型的对抗性攻击)


## 实验方法

我分别使用了 **vit-gpt2-image-captioning** 的图像标记语言模型和 **DETR (End-to-End Object Detection)** 的图像分割模型对图像进行处理，其中

- 图像标记模型能够根据输入图像生成文字；
- 图像分割模型能够根据输入图像生成其中物体的分割。

本文使用的模型链接如下链接所示。

[nlpconnect/vit-gpt2-image-captioning · Hugging Face](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning)

[facebook/detr-resnet-50-panoptic · Hugging Face](https://huggingface.co/facebook/detr-resnet-50-panoptic)

在接下来的实验中，我对这两个模型进行对抗攻击，对抗攻击的概念可见附录。我采用并非是隐秘的攻击，而是大张旗鼓的攻击。即把图像进行叠加，考察这些模型在叠加图像上的表现。

## 原图及 AI 识别结果

下图展示了原图的 AI 识别结果，其中标题行的数字代表叠加强度，$0.0$ 和 $1.0$ 处的数字代表图的叠加强度，如果该数字为  $0.5$ 代表两张图以 $1:1$ 的比例进行叠加；标题行的文字代表 GPT-2 模型生成的图像描述；左图代表输入图；右图代表 DETR 模型给出的分割结果。

从以下结果来看，**AI模型对清晰的、未叠加的图像具有良好的识别能力。**对于车图和树图都给出的良好的结果，但在第三张图中，GPT-2 模型似乎将蝴蝶错认成了小鸟。另外，DETR 模型也没有将红花的轮廓勾勒出来。

![- 0.00 - ['a car that is parked on the side of the road'].jpg](AI%20%E8%AF%86%E5%9B%BE%E7%9A%84%E6%98%93%E9%94%99%E5%9C%BA%E6%99%AF%20217b2bdeb194477687a73178629ec073/-_0.00_-_a_car_that_is_parked_on_the_side_of_the_road.jpg)

![- 1.00 - ['a tree in the middle of a field with a sky background'].jpg](AI%20%E8%AF%86%E5%9B%BE%E7%9A%84%E6%98%93%E9%94%99%E5%9C%BA%E6%99%AF%20217b2bdeb194477687a73178629ec073/-_1.00_-_a_tree_in_the_middle_of_a_field_with_a_sky_background.jpg)

![- 1.00 - ['a small bird is standing in the middle of a flower'].jpg](AI%20%E8%AF%86%E5%9B%BE%E7%9A%84%E6%98%93%E9%94%99%E5%9C%BA%E6%99%AF%20217b2bdeb194477687a73178629ec073/-_1.00_-_a_small_bird_is_standing_in_the_middle_of_a_flower.jpg)

## 叠加图及AI识别结果（错乱）

将车图和树图进行叠加发现，两个模型在叠加图上都失败了。在第一张图中，跑车和大树都没有识别出来；在第二图中，虽然出现了树，但莫名其妙多出来了人。这说明 AI 尚没有叠加的概念，叠加图的纹理会产生对抗攻击。

![- 0.50 - ['a blue umbrella sitting on top of a beach'].jpg](AI%20%E8%AF%86%E5%9B%BE%E7%9A%84%E6%98%93%E9%94%99%E5%9C%BA%E6%99%AF%20217b2bdeb194477687a73178629ec073/-_0.50_-_a_blue_umbrella_sitting_on_top_of_a_beach.jpg)

![- 0.60 - ['a tree with a picture of a man on it'].jpg](AI%20%E8%AF%86%E5%9B%BE%E7%9A%84%E6%98%93%E9%94%99%E5%9C%BA%E6%99%AF%20217b2bdeb194477687a73178629ec073/-_0.60_-_a_tree_with_a_picture_of_a_man_on_it.jpg)

## 叠加图及AI识别结果（正常）

车图和花图的叠加比较正常一些，既有瑕疵，又有有意思的地方。首先，第一张图的描述是车上有花，这是对的。但分割图却无法识别出任何物体；第二张图丢失了跑车的信息，但出乎意料的是， DETR 模型却在叠加图中识别出了红花的轮廓。

![- 0.50 - ['a car that has a bunch of flowers on it'].jpg](AI%20%E8%AF%86%E5%9B%BE%E7%9A%84%E6%98%93%E9%94%99%E5%9C%BA%E6%99%AF%20217b2bdeb194477687a73178629ec073/-_0.50_-_a_car_that_has_a_bunch_of_flowers_on_it.jpg)

![- 0.90 - ['a flower in a vase on a sunny day'].jpg](AI%20%E8%AF%86%E5%9B%BE%E7%9A%84%E6%98%93%E9%94%99%E5%9C%BA%E6%99%AF%20217b2bdeb194477687a73178629ec073/-_0.90_-_a_flower_in_a_vase_on_a_sunny_day.jpg)

## 附录：AI模型的对抗性攻击

对抗性攻击是一种旨在通过对输入数据进行精心设计来欺骗AI模型或使其产生错误预测的技术。这种攻击不是为了损害模型本身，而是为了探索模型的弱点或展示其在特定情况下的脆弱性。对抗性攻击有助于增进对AI模型的理解，并可以用于改进模型的鲁棒性和安全性。

对抗性攻击通常在以下两个方面展开：

1. 生成对抗性样本（Adversarial Examples）：通过对输入数据进行微小的扰动，这些扰动对人类来说几乎不可察觉，但足以导致AI模型产生错误的输出。这些扰动可以是添加噪声、修改像素值等。
2. 对抗性训练（Adversarial Training）：在模型训练阶段，使用生成的对抗性样本来增强模型的鲁棒性。模型在学习如何处理这些对抗性样本的同时，也能提高在正常样本上的性能。

对抗性攻击有几个关键的观点和挑战：

1. 非普遍性：对抗性攻击是一种特殊情况，针对特定模型和特定任务设计的。对抗性样本对一个模型可能有效，但对另一个模型则无效。
2. 可转移性：有些对抗性样本在不同的模型之间是可转移的，即攻击针对一个模型设计的对抗性样本，也可能对另一个模型产生类似的误导效果。
3. 对抗性攻击的防御：研究者一直在努力开发对抗性攻击的防御方法，以提高AI模型的鲁棒性。这可能涉及使用对抗性训练、添加防御层或改进模型架构等。

尽管对抗性攻击可以帮助我们了解AI模型的弱点和改进其安全性，但在使用这些技术时必须谨慎，以避免滥用和潜在的负面后果。