# MLP的相位估计

MLP可以用来估计信号的相位。通过训练带有相位估计目标的MLP,可以学习输入信号和相位之间的复杂非线性关系,从而产生相位估计。

上一句话是 AI 写的，但我有点怀疑 MLP 的能力，于是有此实验。而实验结果说明用 MLP 做相位估计具有较大的潜在风险。 

---
- [MLP的相位估计](#mlp的相位估计)
  - [复数与相位](#复数与相位)
  - [相位估计](#相位估计)
  - [相关概念介绍](#相关概念介绍)
    - [MLP](#mlp)
    - [Phase](#phase)


## 复数与相位

复数是由实部和虚部组成，具有幅值和相位两个属性

$$
\begin{cases}
 i^2 &= -1 \\
 c &= a + i \cdot b \\
 abs(c) &= \sqrt{c \cdot \hat{c}} \\
 phase(c) &= arctan(\frac{b}{a})
\end{cases}
$$

其中，$phase \in (-\pi, \pi)$。在复数平面上均匀采样，使实况和虚部都服从标准正态分布，且相互独立，则这些点的相位分布如下图所示。如图所示，这些点与它们的相位形成一个扭转 $360 \degree$的连续曲面，（为方便计算我将全部相位值除以一个常数，使它被限制在$(-1, 1)$区间之内，但不改变其相互关系）。而由于相位的连续性和周期性，该曲面在一个圆周上必然出现间断，如下图中颜色跳变处所示。这对 MLP 的相位估计造成了困难。

![Untitled](MLP%E7%9A%84%E7%9B%B8%E4%BD%8D%E4%BC%B0%E8%AE%A1%205d5bc9144f444bf980e92deaa55daf58/Untitled.png)

![Untitled](MLP%E7%9A%84%E7%9B%B8%E4%BD%8D%E4%BC%B0%E8%AE%A1%205d5bc9144f444bf980e92deaa55daf58/Untitled%201.png)

## 相位估计

我通过 pytorch 构造了简单的 MLP 网络，它的结构如附表所示。经过 2000 次训练后，其损失函数不断下降且趋于稳定，这说明网络参数已经收敛于最优估计。

![Untitled](MLP%E7%9A%84%E7%9B%B8%E4%BD%8D%E4%BC%B0%E8%AE%A1%205d5bc9144f444bf980e92deaa55daf58/Untitled%202.png)

接下来，我们使用独立的测试集对相位估计的准确性进行测试，得到误差图如下。其中左图是在三维空间内的误差图，每个点的纵坐标代表估计出的相位值，其颜色代表误差大小；右图的纵坐标是误差大小。可以看到，在相位跳变处，MLP几乎无法预测。

![Untitled](MLP%E7%9A%84%E7%9B%B8%E4%BD%8D%E4%BC%B0%E8%AE%A1%205d5bc9144f444bf980e92deaa55daf58/Untitled%203.png)

![Untitled](MLP%E7%9A%84%E7%9B%B8%E4%BD%8D%E4%BC%B0%E8%AE%A1%205d5bc9144f444bf980e92deaa55daf58/Untitled%204.png)

值得注意的是，这里我并没有冤枉 MLP 网络，因为一种可能的辩护是说，由于相位的周期性，因此在跳变点处可能存在$\pi \neq -\pi$的争议现象。为了避免这种现象，我在计算误差时将误差与$2\pi$进行取模，从而规避这种情况

```python
df['diff'] = df['predPhase'] - df['phase']
df['diff'] = df['diff'].map(lambda e: e % (np.pi * 2))
df['diff'] = df['diff'].map(lambda e: np.min((e, np.pi * 2 - e)))
```

但即使在这种情况下，误差的直方图仍然如下图所示，集中在 $0.1$ 处，这代表 $10\%$ 的相位误差，如左图所示。另外在跳变处，相位误差几乎达到了 $100\%$，如右图所示。这显然是不可接受的结果。这说明用 MLP 做相位估计具有较大的潜在风险。 

![Untitled](MLP%E7%9A%84%E7%9B%B8%E4%BD%8D%E4%BC%B0%E8%AE%A1%205d5bc9144f444bf980e92deaa55daf58/Untitled%205.png)

![Untitled](MLP%E7%9A%84%E7%9B%B8%E4%BD%8D%E4%BC%B0%E8%AE%A1%205d5bc9144f444bf980e92deaa55daf58/Untitled%206.png)

---

## 相关概念介绍

此部分为 AI 续写。

### MLP

A Multi-Layer Perceptron (MLP) is a type of feedforward artificial neural network. It consists of multiple layers of nodes between the input and output layers, with each layer using a nonlinear activation function. MLPs use a supervised learning technique called backpropagation for training the network, where the weights and biases of the network are iteratively adjusted to minimize the error between the network's predicted output and the known target values. Due to their ability to learn complex nonlinear relationships between inputs and outputs, MLPs are commonly used for tasks such as classification and regression.

```jsx
Net(
  (mlp): MLP(
    (0): Linear(in_features=2, out_features=4, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=4, out_features=8, bias=True)
    (4): LeakyReLU(negative_slope=0.01, inplace=True)
    (5): Dropout(p=0.0, inplace=True)
    (6): Linear(in_features=8, out_features=4, bias=True)
    (7): LeakyReLU(negative_slope=0.01, inplace=True)
    (8): Dropout(p=0.0, inplace=True)
    (9): Linear(in_features=4, out_features=1, bias=True)
    (10): Dropout(p=0.0, inplace=True)
  )
  (sig): Tanh()
)
```

### Phase

Phase estimation is an important task in many signal processing applications. Given a noisy input signal, the goal is to estimate the phase of the underlying "clean" signal. Phase estimation is challenging due to the periodic and nonlinear nature of the phase. There are many approaches to phase estimation, including the Hilbert transform, minimum mean squared error estimation, and neural network-based methods.