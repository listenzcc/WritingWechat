# 记MLP的一次回归失败

多层感知机（MLP）是常用的神经网络结构，在数学上证明它可以用来表达几乎全部连续函数。

但由于函数的复杂性和训练算法设计的不足，它有时候并不能很好地完成给定的任务。

本文是记录一次用 MLP 进行回归的失败尝试，代码可见我的 GITHUB 仓库

[https://github.com/listenzcc/MLP-notebook](https://github.com/listenzcc/MLP-notebook "https://github.com/listenzcc/MLP-notebook")

---
- [记MLP的一次回归失败](#记mlp的一次回归失败)
  - [回归命题](#回归命题)
  - [网络设置](#网络设置)
  - [实验结果](#实验结果)


## 回归命题

我要解决的问题很简单，是想设计一个 MLP 网络$\Psi$，用它完成一个回归任务

$$
f(t) = \Psi(g(t))
$$

其中映射两端的函数都是连续的复变函数，它们具有相同的形式

$$
\begin{cases}
f(t) &= r(t) \cdot e^{-j (\omega t + \phi)} \\
r(t) &= k \cdot t + b
\end{cases}
$$

其中除参数$t$之外，其余参数均为常数。因此，不同的参数组合可以用来构造回归任务的自变量和因变量函数

$$
\begin{cases}
f(t) := (k_1, b_1, \omega_1, \phi_1) \\
g(t) := (k_2, b_2, \omega_2, \phi_2) \\
\end{cases}
$$

稍加思考便知，形如以上的函数在复平面上构成一组螺旋线，如下图所示。

![Untitled](%E8%AE%B0MLP%E7%9A%84%E4%B8%80%E6%AC%A1%E5%9B%9E%E5%BD%92%E5%A4%B1%E8%B4%A5%20ee7c96cd4e9249519f582d03922c87c0/Untitled.png)

构造数据的代码如下

```python
def mk_dataframe(x, y, r, theta, name='noname'):
    '''
    Make dataframe for the dataset
    '''
    df = pd.DataFrame()
    df['x'] = x
    df['y'] = y
    df['theta'] = theta
    df['r'] = r
    df['name'] = name
    df['idx'] = range(len(df))
    return df

# Setup
# 'sample' mark means using the data to train the network
n = 5000
m = 2000
sample = np.random.choice(range(n), m, replace=false)

# src
theta = np.linspace(0, np.pi * 5, n)
r = np.linspace(0.2, 0.8, n)
x = r * np.cos(theta)
y = r * np.sin(theta)
df1 = mk_dataframe(x, y, r, theta, 'src')
df1['sample'] = 'na'
df1.loc[sample, 'sample'] = 'sample'

# target
theta = np.linspace(0, np.pi * 6, n)
r = np.linspace(0.1, 0.9, n)
x = r * np.cos(theta)
y = r * np.sin(theta)
df2 = mk_dataframe(x, y, r, theta, 'target')
df2['sample'] = 'na'
df2.loc[sample, 'sample'] = 'sample'
```

## 网络设置

螺旋线的形式看上去很简洁，因此我构造了一个简单的 MLP 网络，试图用它对函数进行回归。基于回归计算的特性，选择 MSE 作为损失函数。因此，网络构造和训练使用的代码如下

```python
# Network generation
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.mlp = torchvision.ops.MLP(2, [4, 3, 2], activation_layer=nn.LeakyReLU)
        self.sig = nn.Tanh()

    def forward(self, x):
        return self.sig(self.mlp(x))

net = Net().cuda()

# Network training
lr = 1e-2
optimizer = torch.optim.AdamW(net.parameters(), lr)
criterion = nn.MSELoss()
```

## 实验结果

然而，实验结果令我有点吃惊，因为网络参数虽然收敛，但并没有完成回归任务。

| Step | Loss(train) | Loss(test) |
| ---- | ----------- | ---------- |
| 0960 | 0.0280      | 0.0251     |
| 0970 | 0.0213      | 0.0251     |
| 0980 | 0.0223      | 0.0251     |
| 0990 | 0.0234      | 0.0254     |

回归效果如下图所示，图中红色和绿色曲线分别代表回归方程的左侧和右侧变量，图中蓝色和紫色曲线分别代表回归结果和差异曲线。可见，回归效果不佳。

![Untitled](%E8%AE%B0MLP%E7%9A%84%E4%B8%80%E6%AC%A1%E5%9B%9E%E5%BD%92%E5%A4%B1%E8%B4%A5%20ee7c96cd4e9249519f582d03922c87c0/Untitled%201.png)

接下来，为了考察效果不佳的位置在哪里，我将点云顺序作为第三个维度绘制出来，这样相同层面的数据之间就具有可比性，从下面的图中可以看到，回归结果虽然差异较大，但 MLP 仍然是以特定的“螺旋形”进行回归的，只是没有得到最正确的那组参数。

但是我最终也没有想明白这其中的数学原理是什么。不兴趣的小伙伴可以移步我的代码库，其中有可交互的 3D 图

[Experiment of MLP](https://listenzcc.github.io/MLP-notebook/ "Experiment of mlp")

下图中不同颜色的曲线与上图中的意义相同。

![Untitled](%E8%AE%B0MLP%E7%9A%84%E4%B8%80%E6%AC%A1%E5%9B%9E%E5%BD%92%E5%A4%B1%E8%B4%A5%20ee7c96cd4e9249519f582d03922c87c0/Untitled%202.png)

![Untitled](%E8%AE%B0MLP%E7%9A%84%E4%B8%80%E6%AC%A1%E5%9B%9E%E5%BD%92%E5%A4%B1%E8%B4%A5%20ee7c96cd4e9249519f582d03922c87c0/Untitled%203.png)

接下来，我们忽略曲线差异，用不同的颜色代表点的顺序，如下图所示。

![Untitled](%E8%AE%B0MLP%E7%9A%84%E4%B8%80%E6%AC%A1%E5%9B%9E%E5%BD%92%E5%A4%B1%E8%B4%A5%20ee7c96cd4e9249519f582d03922c87c0/Untitled%204.png)

![Untitled](%E8%AE%B0MLP%E7%9A%84%E4%B8%80%E6%AC%A1%E5%9B%9E%E5%BD%92%E5%A4%B1%E8%B4%A5%20ee7c96cd4e9249519f582d03922c87c0/Untitled%205.png)