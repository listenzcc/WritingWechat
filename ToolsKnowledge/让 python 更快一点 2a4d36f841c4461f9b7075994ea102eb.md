# 让 python 更快一点

本文要解决的问题是如何在 Python 环境下，在 100 毫秒之内稳定处理随机的大量 4k （$3840 \times2160$）图像线性运算。由于图像信息具有特殊性，我们通过并行计算和GPU加速的方式进行优化。本文对这两种加速方法都进行尝试，实验结果表明，Python加速的有效手段并非并行计算，而是装到GPU中进行计算，加速效果可以达到 $2$ 倍。

接下来，我决定再进一步，将数据全部放入 GPU 中进行计算，省去临时装载和读取的开销，实验结果如下图中 cuda 所示。这样做的加速效果可以达到 $200 \sim 1200$ 倍不等，但计算负荷越高则加速效果越差。

本文开源代码可见我的 Github 仓库
[python-speed-validation](https://github.com/listenzcc/python-speed-validation)

---
- [让 python 更快一点](#让-python-更快一点)
  - [测试任务](#测试任务)
  - [并行计划的加速](#并行计划的加速)
  - [Cuda 的加速](#cuda-的加速)


## 测试任务

想象你面对一个随机的图像序列，现在的任务是图像以固定的间隔呈现，比如说这个间隔是$100ms$。现在的任务是对相邻图像进行插值，形成一段更加连续的时间序列

$$
p_i^r = r \cdot p_i + (1-r) \cdot p_{i+1}
$$

其中，$p_i = (3840 \times2160)$代表随机 4k 图像，$r\in(0, 0.1, 0.2, \dots, 0.9)$代表图像之间的比例系数。由于图像序列是随机生成的，在运行时也可能有其他图像插入这个序列的任意位置，因此需要对这些插值进行实时计算。

![Untitled](%E8%AE%A9%20python%20%E6%9B%B4%E5%BF%AB%E4%B8%80%E7%82%B9%202a4d36f841c4461f9b7075994ea102eb/Untitled.png)

## 并行计划的加速

程序加速的思路无非是简化算法和优化计算流程，由于本例的算法比较简单因此优化的空间不大。因此，将计算过程并行化是首当其冲的加速思路。另外，由于图像信息具有特殊性，我们还可以通过GPU加速的方式进行优化。本文对这两种加速方法都进行尝试，参与比较的方法列写如下，

- CPU，这代表未经任何优化的循环计算；
- GPU，这代表计算时将图像数据放入GPU进行计算；
- parallel，这代表使用 joblib 进行并行计算。

[https://github.com/joblib/joblib](https://github.com/joblib/joblib)

[PyTorch](https://pytorch.org/)

在测试代码中，测试负载设置如下

- 不断增加$r$值的范围，分别设置为 $10, 20, 30, 40$ ，用于模拟越来越精细的图像插值精度，也代表越来越大的计算负荷；
- 这个过程重复 $10$ 次，将这些任务叠加用来测试程序执行时间；
- 统计程序执行时间，用于方法之间的比较。

CPU、GPU和parallel方法之间的速度比较情况如下图所示，

- 在高负荷的情况下，parallel方法快于CPU方法；
- 在低负载情况下，parallel方法不仅没有优势，甚至更慢；
- 将数据装入GPU的方法在任何负载下都更快。

实验结果表明，Python加速的有效手段并非并行计算，而是装到GPU中进行计算，加速效果可以达到 $2$ 倍。

![summary.jpg](%E8%AE%A9%20python%20%E6%9B%B4%E5%BF%AB%E4%B8%80%E7%82%B9%202a4d36f841c4461f9b7075994ea102eb/summary.jpg)

## Cuda 的加速

接下来，我决定再进一步，将数据全部放入 GPU 中进行计算，省去临时装载和读取的开销，实验结果如下图中 cuda 所示。这样做的加速效果可以达到 $200 \sim 1200$ 倍不等，但计算负荷越高则加速效果越差。

![summary-1.jpg](%E8%AE%A9%20python%20%E6%9B%B4%E5%BF%AB%E4%B8%80%E7%82%B9%202a4d36f841c4461f9b7075994ea102eb/summary-1.jpg)