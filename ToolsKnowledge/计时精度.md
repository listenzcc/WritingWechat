## 计时精度

都说`LINUX`比`WINDOWS`操作系统更适合做工程开发，
这也许是谣言，
但绝非空穴来风，
本文就是一例。

设想一个典型场景，
你需要让计算机每隔一段时间做一件事情，
比如显示一张图片什么的，
那么就要用到计时器。

当这段时间短到毫秒级的时候，
问题就来了。

---

-   [计时精度](#计时精度)
-   [问题描述](#问题描述)
-   [WINDOWS 的睡眠](#windows-的睡眠)
-   [LINUX 的睡眠](#linux-的睡眠)
-   [WINDOWS 的问题出在哪里](#windows-的问题出在哪里)

## 问题描述

长话短说，
如果你需要一个程序每隔`100`毫秒做一件事情，
在最典型的情况下，你需要这样做

```cpp
// 每 100 毫秒执行一次特定任务的伪代码
// 版本 1.0

(下面的代码循环 n 次)
{
    程序睡眠 100 毫秒;
    执行某个功能;
}
```

很简单，
但有风险。
因为如果执行某个功能需要一定的时间，
那么这个时间就会占用下一个`100`毫秒的一段时间，
从而使间隔增加。
多线程可以减小这个风险

```cpp
// 每 100 毫秒执行一次特定任务的伪代码
// 版本 2.0

(下面的代码循环 n 次)
{
    程序睡眠 100 毫秒;
    打一个新线程(执行某个功能);
}
```

非常完美，
**只要**程序能在睡下`100`毫秒后**准时**醒来。

这里可以先说结论，
在这一点上，
`LINUX` 比 `WINDOWS` 做得要好。

## WINDOWS 的睡眠

为了测试`WINDOWS`的准时性能，
我使用以下代码

```cpp
// C++ script to sleep 100 milliseconds for 30 times,
// OS: WINDOWS

#include <iostream>
#include <windows.h>

int main()
{
    clock_t START, END, start, end;

    START = clock();

    for (int i = 0; i < 30; i++) {
        start = clock();

        Sleep(100);

        end = clock();
        END = clock();

        float delay = end - start;

        printf("%02d sleeps %f milliseconds\n", i, delay);
    }

    printf("Total costs %f milliseconds\n", ((float)END - (float)START));

}
```

测试结果如下，

![sleep-1](./sleep-1.png)

每次让它睡`100`毫秒它都会吞掉我`5`毫秒。
为了确保这个误差不是由时间获取时的延时造成的，
我同时统计了全部`30`次总误差，
结果显示它总共多睡了`150`毫秒，
结果与单次的`5`毫秒是吻合的。

## LINUX 的睡眠

再来看看`LINUX`的结果，

```cpp
// C++ script to sleep 100 milliseconds for 30 times,
// OS: LINUX

#include <sys/time.h>
#include <stdio.h>
#include <unistd.h>
int main()
{
    struct timeval START, END, start, end;

    long mtime, seconds, useconds;

    int timeSleep = 100 * 1000;

    gettimeofday(&START, NULL);

    for (int i=0; i<30; i++) {
            gettimeofday(&start, NULL);
            usleep(timeSleep);
            gettimeofday(&end, NULL);
            gettimeofday(&END, NULL);

            seconds  = end.tv_sec  - start.tv_sec;
            useconds = end.tv_usec - start.tv_usec;

            mtime = ((seconds) * 1000 + useconds/1000.0) + 0.5;

            printf("%02d: %ld milliseconds\n", i, mtime);
    }


    seconds  = END.tv_sec  - START.tv_sec;
    useconds = END.tv_usec - START.tv_usec;

    mtime = ((seconds) * 1000 + useconds/1000.0) + 0.5;
    printf("Total costs %ld milliseconds\n", mtime);

    return 0;
}

```

结果如下

![sleep-2](./sleep-2.png)

它睡得很准确，完了。

## WINDOWS 的问题出在哪里

至于`WINDOWS`的问题出在了哪里，
实在是一个无从下手的问题，
官方材料很少提及这个问题。

但是我可以**猜**，
首先，
我让它睡`1`毫秒，
结果如下

![sleep-3](./sleep-3.png)

结果很有意思，
它睡了`15`毫秒。

接下来，
我让它睡`89`毫秒，
结果如下

![sleep-4](./sleep-4.png)

在多数情况下，
它会睡`90`毫秒。

注意到这样的关系

$$15 * 6 = 90$$
$$15 * 7 = 105$$

所以，
我猜`WINDOWS`的底层是以`15`毫秒大小的时间片进行资源分配的，
一旦某个程序申请睡眠或从内存里转移，
那么它的行为就会被操作系统记住，
进入等待轮询的状态，
而操作系统是以`15`毫秒为基础的时间片进行工作的，
什么时候再轮询到它呢？
一定是`15`毫秒的整数倍之后。

当然，这些只是猜测。
本文主要是把坑写出来，
免得读到的人之后在这里浪费时间。

最后，
这个机制是可以利用的吗？
答案也是否定的，
因为从目前最简单的验证结果来看，
这个`15`毫秒并不精确，
甚至它的存在也几乎得不到保证。
所以这就是一个坑，
而不是一个`TRICK`。
