# 隐空间的可区分性

前文涉及了多元高斯分布以及它的协方差矩阵的逆，列出了隐变量的二次方程，还从后验概率的角度阐述了它与sigmoid损失函数之间的关系。本文继续在这个隐空间中，阐述隐变量的可分性，尝试从最肤浅的角度回答一个基本问题，那就是“sigmoid损失函数是如何区分不同类别的”，暨二次方程的一次和二次项在何时生效的问题。分析表明，我们更希望隐变量的一次项起主导作用，这也是深度神经网络最后一层通常使用全连接层的原因。

---
[toc]

## 协方差矩阵与概率密度

考察多元高斯分布的概率密度函数，为了简单起见，我将协方差矩阵的逆成为核

$$
p(x|C_1)\propto \exp{-(x-\mu_1)^T\Sigma_1(x-\mu_1)}
$$

由于核是具有对称性的实矩阵，因此它总能够分解成如下形式，

$$
\Sigma = U^T\Lambda V
$$

其中，$\Lambda$代表特征值的对角矩阵，$U=V$代表正交的特征向量列矩阵。而且它至少是半正定的

$$
\Lambda_{ii} \ge 0
$$

考虑新变量$z$，使得

$$
z = (x-\mu_1)^T\Sigma_1(x-\mu_1)
$$

由概率的性质可得，当$x$出现的概率较高时，$z$值较小，越接近$0$代表概率越高；反之，当$x$出现的概率较小时，$z$值较大，越远离$0$代表概率越小，如下图所示

![Untitled](%E9%9A%90%E7%A9%BA%E9%97%B4%E7%9A%84%E5%8F%AF%E5%8C%BA%E5%88%86%E6%80%A7%202ea8d5a119f24257a9b484a4af0f6a0c/Untitled.png)

## 二次的隐变量方程

接下来，我将之前分析得到的隐变量的二次函数列写如下

$$
\psi = 
x^T(\Sigma_2-\Sigma_1)x + 2(\mu_1^T\Sigma_1-\mu_2^T\Sigma_2)x -\mu_1^2\Sigma_1\mu_1 +\mu_2^2\Sigma_2\mu_2
$$

不难发现，对它的分析可以从如下两个方面进行。

## 依靠均值差异的分类

首先，当两个类别的均值差异较大，而协方差差异较小时，隐变量的一次项占主导

$$
\begin{cases}
(\mu_1^T\Sigma_1-\mu_2^T\Sigma_2)x >0, x\in C_1 \\
(\mu_1^T\Sigma_1-\mu_2^T\Sigma_2)x <0, x\in C_2 \\
\Sigma_1 = \Sigma_2
\end{cases}
$$

![Untitled](%E9%9A%90%E7%A9%BA%E9%97%B4%E7%9A%84%E5%8F%AF%E5%8C%BA%E5%88%86%E6%80%A7%202ea8d5a119f24257a9b484a4af0f6a0c/Untitled%201.png)

## 依靠协方差差异的分类

其次，当两个类别的均值差异较小，而协方差差异较大时，隐变量的二次项占主导。

$$
\begin{cases}
\mu = 0\\
x^T(\Sigma_2-\Sigma_1)x > 0, x\in C_1 \\
x^T(\Sigma_2-\Sigma_1)x < 0, x\in C_2
\end{cases}
$$

![Untitled](%E9%9A%90%E7%A9%BA%E9%97%B4%E7%9A%84%E5%8F%AF%E5%8C%BA%E5%88%86%E6%80%A7%202ea8d5a119f24257a9b484a4af0f6a0c/Untitled%202.png)
![Untitled](%E9%9A%90%E7%A9%BA%E9%97%B4%E7%9A%84%E5%8F%AF%E5%8C%BA%E5%88%86%E6%80%A7%202ea8d5a119f24257a9b484a4af0f6a0c/Untitled%203.png)

虽然它占主导，但无法完成分布中心的分类任务，这是由于两类的隐变量在这个区域出现重叠。因此，这种情况下，分类器难以对类别进行区分。也因此，我们更希望隐变量的一次项起主导作用，这也是深度神经网络最后一层通常使用全连接层的原因。