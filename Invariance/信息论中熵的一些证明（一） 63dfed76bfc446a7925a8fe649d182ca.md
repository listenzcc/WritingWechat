# 信息论中熵的一些证明（一）

本文提供一些信息熵的有用证明，供日后查阅。

---

-   [信息论中熵的一些证明（一）](#信息论中熵的一些证明一)
    -   [Shannon Entropy 的极值](#shannon-entropy-的极值)
    -   [Cross Entropy 的极值](#cross-entropy-的极值)
    -   [简单例子](#简单例子)

## Shannon Entropy 的极值

从熵的定义出发

$$
E(X)=\mathbb{E}_{x \sim P}[I(p)]= - \int_{-\infty}^{\infty} p(x) \cdot \log{p(x)} dx
$$

再加上概率密度函数的归一化约束

$$
1 - \int_{-\infty}^{\infty}p(x)dx=0
$$

可以构造拉格朗日函数

$$
\mathcal{L}= - \int_{-\infty}^{\infty} p(x) \cdot \log{p(x)} dx + \lambda \cdot (1 - \int_{-\infty}^{\infty}p(x)dx)
$$

其中，归一化约束的系数为待定常数。对概率密度函数求偏导，可得

$$
\frac{\partial}{\partial p} \mathcal{L} = - \log{p(x)} - 1 - \lambda
$$

求二阶偏导，可得

$$
\frac{\partial^2}{\partial p^2}\mathcal{L}=-\frac{1}{p(x)}
$$

由于二阶偏导恒小于零，且当概率密度函数为“平均分布”时该函数取极大值，因此可以认为当概率密度函数为“平均分布”时，Shannon Entropy 达到最大。也就是说，如果一个符号的概率密度函数越平均，则它越难预测，对这个符号进行观测时，能够解决的信息量也就越大。

$$
p(x) = \mathcal{C}
$$

另外，由于上述求导方法并不严谨，因此对它进行必要的补充。不严谨的地方在于概率密度函数作为整体，在某个位置的扰动必然会影响其他位置，因此，不能简单假设它们是独立的。

$$
\frac{\partial}{\partial p} \mathcal{L} = \int_y(- \log{p(y)} - 1) \cdot \mathcal{J_{xy}} - \lambda \cdot \mathcal{J_{xy}} dy
$$

其中，$\mathcal{J}_{xy}$代表两个位置下概率密度函数的 Jacobian 行列式。由于归一化函数关系的存在，这些行列式总可以写成对角矩阵的形式，且对角线上的值为 1 和 -1。此时，虽然目前的证明不保证该式为零时有唯一解，但仍然不妨碍平均分布是该方程的一个平凡解。

## Cross Entropy 的极值

交叉熵的定义如下

$$
CE(X, Y)=\mathbb{E}_{x \sim P}[I(q)] = - \int p(x) \cdot \log{q(x)} dx
$$

同样构造拉氏方程

$$
\mathcal{L}= - \int_{-\infty}^{\infty} p(x) \cdot \log{q(x)} dx + \lambda \cdot (1 - \int_{-\infty}^{\infty}q(x)dx)
$$

其偏微分为

$$
\frac{\partial}{\partial q} \mathcal{L} = - \frac{p(x)}{q(x)} - \lambda
$$

其二阶偏微分为

$$
\frac{\partial^2}{\partial q^2}\mathcal{L}=\frac{p(x)}{q^2(x)}
$$

由于两个分布都受到归一化约束，因此当且仅当两个分布“完全一致”时，函数取极值。又由于其二阶偏导函数非负，因此该极值为极小值。在考虑到 Jacobian 行列式后，这同样是一个平凡解。与 Shannon Entropy 时相同。

## 简单例子

接下来，对 Shannon Entropy 的计算举一个简单的例子。对于高维分布总有归一化约束存在，

$$
\sum_i^{N} p_i = 1
$$

因此对们进行简单的“合并”，或者说“特征提取”。提取的方法是通过聚类方法将这些分布聚成 3 组，对 3 组分布求和

$$
q_j = \sum_{i \in Q_j} p_i, j\in1, 2, 3
$$

则新的概率密度函数只有 3 个维度，且三个变量之和恒为 1。这就相当于三维空间中的一个平面

$$
p_{xyz} | x + y + z = 1, x,y,z > 0
$$

则之前的 Perlin 噪声可以在这个空间中表示成如下平面。可见这些噪声基本上铺满了整个三角形空间，且越靠近中央其信息熵越高，这与之前的推论是一致的。

![newplot (19).png](<%E4%BF%A1%E6%81%AF%E8%AE%BA%E4%B8%AD%E7%86%B5%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AF%81%E6%98%8E%EF%BC%88%E4%B8%80%EF%BC%89%2063dfed76bfc446a7925a8fe649d182ca/newplot_(19).png>)

![newplot (20).png](<%E4%BF%A1%E6%81%AF%E8%AE%BA%E4%B8%AD%E7%86%B5%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AF%81%E6%98%8E%EF%BC%88%E4%B8%80%EF%BC%89%2063dfed76bfc446a7925a8fe649d182ca/newplot_(20).png>)

而如果信号分布过于接近，它们在整个空间中只会占据一小部分

![newplot (21).png](<%E4%BF%A1%E6%81%AF%E8%AE%BA%E4%B8%AD%E7%86%B5%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AF%81%E6%98%8E%EF%BC%88%E4%B8%80%EF%BC%89%2063dfed76bfc446a7925a8fe649d182ca/newplot_(21).png>)

下面选择一张实际的图，将它的行当作一些信号，则这些信号在概率空间中的分布如下，一般来讲，它们都会呈现出一定的形状，这些分布的形状就是 ML 领域要学到的东西。

![newplot (22).png](<%E4%BF%A1%E6%81%AF%E8%AE%BA%E4%B8%AD%E7%86%B5%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AF%81%E6%98%8E%EF%BC%88%E4%B8%80%EF%BC%89%2063dfed76bfc446a7925a8fe649d182ca/newplot_(22).png>)

![newplot (23).png](<%E4%BF%A1%E6%81%AF%E8%AE%BA%E4%B8%AD%E7%86%B5%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AF%81%E6%98%8E%EF%BC%88%E4%B8%80%EF%BC%89%2063dfed76bfc446a7925a8fe649d182ca/newplot_(23).png>)

![newplot (24).png](<%E4%BF%A1%E6%81%AF%E8%AE%BA%E4%B8%AD%E7%86%B5%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AF%81%E6%98%8E%EF%BC%88%E4%B8%80%EF%BC%89%2063dfed76bfc446a7925a8fe649d182ca/newplot_(24).png>)

![Canaletto Regatta on Grand Canal detail.jpg](%E4%BF%A1%E6%81%AF%E8%AE%BA%E4%B8%AD%E7%86%B5%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AF%81%E6%98%8E%EF%BC%88%E4%B8%80%EF%BC%89%2063dfed76bfc446a7925a8fe649d182ca/Canaletto_Regatta_on_Grand_Canal_detail.jpg)
