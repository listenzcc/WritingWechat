## 知识、经验和随机性

本文试图说明为什么以及如何使用线性模型对随机变量进行解构。

---

- [知识、经验和随机性](#知识经验和随机性)
- [知识与经验](#知识与经验)
- [随机性](#随机性)
- [解构随机性](#解构随机性)

## 知识与经验

之前我们指出，
发明数列的重大作用之一是为了对实数进行描述。

对于特定的重要实数，
数学家给出了无数种或简单或复杂，
并且极其有用的数列表达方式。

$$y=f(x)$$

从实数的特定性质出发，
找到一种特定的表达方式的过程属于创造知识的过程。
比如

$$e=\sum_{i=0}^\infty (1+\frac{1}{n})^n$$

以及更加神奇的计算圆周律的天才公式

![Compute PI](./compute-pi.png)

有了这类知识，
无论数列的形式有多么复杂，
都不妨碍人们使用它来计算数值，
因此，之后的工作就属于经验的范畴。

这里就涉及两个相对的方向，

- 从特定的实数出发，利用特定的数列对它进行逐次逼近的描述的过程，是知识的创造过程；
- 反过来，利用数列求解实数的过程，即是利用知识的经验过程。

将知识与经验的范畴进行厘清之后，
就可以稍微放下特定的实数，
转而对更加现实的，
随机问题进行考察。

## 随机性

除了特定的实数之外，
我们往往还会遇到许多取值不固定的测量值，
比如每天的气温、股票价格等等。

这些数值具有的共性是它们服从特定的分布，
在每次观测中，它们必然会取某个值，
但具体取值是随机的，
因此称为随机变量。

$$\{y_i|y_i \in Y\}$$

针对这些随机变量的知识极具价值，
这些知识可以用来帮助我们确定“某一次观测”的概然取值。
在统计学中，
我们比较关注数值的中心（均值、中值等）和偏离中心的程度（方差，高阶矩）等，
称为统计量。

在分布已知的情况下，
这些统计量是稳定的，甚至是固定的。
我们能够从这种全面的知识中，
推导出所需要的统计量。

之前写过“概然而非必然的世界”系列，
就涉及到了这些问题，
此处不再赘述。

而在分布未知的情况下，
就需要找到新的知识对它进行表达。

## 解构随机性

在此，我们对随机性稍加分析，
不妨假设它们按照来源不同，可以分为两类

- 第一类是自发的、系统性的、难以避免的随机性，
  比如热运动、混沌性等，
  带有随机噪声性质的随机性；

  $$\epsilon$$

- 另一类是受到其他复杂因素影响，
  从而产生的随机性，
  它们又能够分为两种，

  - 一种是受到某种因素的影响程度是固定的；

    $$\alpha_i \cdot X_i$$

  - 另一种是受到影响的程度也是随机的。

    $$\Beta_j \cdot X_j$$

    其中$\Beta_j \propto \mathcal{N}(b_j, \delta_j^2)$

在这样的假设之下，可以对随机变量进行描述，即线性模型

$$Y=\sum_{i} \alpha_i \cdot X_i + \sum_{j} \Beta_i \cdot X_j + \epsilon$$

其中，$X$变量是可观测变量，通过它们可以对随机变量$Y$的取值进行计算。
$\alpha$参数对应称为“固定效应模型”；
$\Beta$参数对应称为“随机效应模型”；
两者并列称为“混合效应模型”。

建立这样的模型需要假设，
比如

$$股票价格=\alpha \cdot 员工加班时长 + \beta \cdot 员工摸鱼时长 + \gamma \cdot 火星距离地球距离 + \psi \cdot 原材料价格 + \epsilon$$

方程右边的变量可以观测，
而这些观测值可以通过给定的系数（$\alpha, \beta, \dots$）对左边的变量进行估计。

这就形成了一个关于“股票价格”的信念，
要想它变成“知识”，
就需要在大胆假设之后，
进行小心求证。

求证什么呢？
求证两个问题

1. 哪些系数的值不等于零；
2. 这些系数的组合是否能够有效地估计目标值。

前者通过假设检验（方差分析）来解决，
后者则涉及机器学习中线性回归的基本理论。
但不管具体的方法是什么，
从大的范畴来讲，
所提出的模型假设是知识的前身，
而模型的参数是否得当，以及它们的具体取值则属于经验的范畴。