## 有图必有谱

谱聚类是针对“图”的节点分割算法。
理想的谱聚类需要解决NP-Hard问题：
分割导致解空间不连续；
而节点搜索导致计算规模随着数据规模而扩大。
因此，谱聚类算法是寻找一个可行的近似解。

---

- [有图必有谱](#有图必有谱)
  - [谱聚类的问题描述](#谱聚类的问题描述)
    - [图的拉普拉斯矩阵](#图的拉普拉斯矩阵)
    - [指示向量](#指示向量)
    - [NP-Hard](#np-hard)
  - [谱聚类的近似求解](#谱聚类的近似求解)
    - [特征值分解](#特征值分解)
    - [K均值](#k均值)
    - [误差估计](#误差估计)

### 谱聚类的问题描述

我们希望有这样一种算法，
它能够根据图中各个节点的连接度，
对它们进行分割。
这样，在对它们进行可视化的时候，
用户可以不再纠结把每个节点放在哪里，
而是可以通过一个简单的物理力学模拟，
达到将它们放置在合适位置的效果。

【这是一段棒到不行的视频】

感兴趣可以移步我的交互式[Notebook](https://observablehq.com/@listenzcc/spectral-clustering "Notebook")。

可以看到，
用户只需要做一些简单的拖拽，
就可以得到一张看得过去的可视化图结构。

#### 图的拉普拉斯矩阵

对于$N$个顶点构成的连接图，
总可以用$W \in R^{N \times N}$矩阵表示这些顶点之间的连接关系。
其中，$W_{ij}$代表节点$i, j$之间的连接强度。
此时，由于缺乏合适的定义方式，
该矩阵主对角线上的值均未定，
我们暂定它们为零。

为了补全矩阵对角上的值，
我们计算每个节点的“连接度”，
构造新的对角阵$A \in R^{N \times N}$

$$A_i = \sum_{j=1}^{N} W_{ij}$$

此时，我们可以通过$A$矩阵补全$W$矩阵，
从而构造“拉普拉斯”矩阵$L$

$$L = A - W$$

这个矩阵是谱聚类的基础。

#### 指示向量

先不纠结物理意义，
我们直接构造一个指示向量$V_k \in R^N$

$$V_{ki} = x_i \in \{0, c_k\}$$

这是一种比较绕的表示方法。
简单来说，
它是一个由$0$和$c_k$组成的$N$维列向量。
其中，$c_k$代表正常数，在数值上它等于$V_{ik}$向量的非零值数量的倒数

$$c_k = \frac{1}{\sum_i \delta(x_i, 0)}$$

其中，$\delta(\cdot, \cdot)$代表狄利克雷函数。

基于这个指示向量，
我们构造它的二次型

$$V_k^T \cdot L \cdot V_k$$

如果你熟悉矩阵的乘法规则，
可以容易看到二次型的值即是矩阵$L$相应行和列所有元素之和

![VtLV](VtLV.png)

它具有极其明确的物理意义，
代表按照指示向量$V_k$的要求对图进行切割之后，
所有不同组的节点之间的连接强度之和。

$$V_k^T \cdot L \cdot V_k = \frac{1}{c_k} \sum_{x_i \neq 0, x_j = 0} w_{ij}$$

对于$K$个类别的分割，
谱聚类问题可以等价于找到$K$个相互独立的$V_k$指示向量，
使其满足

$$\hat{V} = argmin_V tr(\sum_{k=1}^{K} V_k^T \cdot L \cdot V_k)$$

其中，$tr(\cdot)$代表求矩阵的迹，
我们同时要求指示向量相互正交，$V_i \cdot V_j = 0, i \neq j$。

它的物理意义为

> 找到图的$K$组的分割，这个分割满足两个条件
>
> 1. 不同组节点之间的连接强度尽量小，即切掉尽量少（不重要）的边；
> 2. 组内节点之间的连接强度尽量大，即保留尽量多（重要）的边。

根据这些指示向量的非零值，
我们可以对不同类的节点进行染色，
得到的效果如下图所示

![Cluster1](Cluster1.png)

图中的颜色代表不同组的节点，
线条代表节点之间的连接。

#### NP-Hard

回顾我们对指示向量的要求，可以概括为两点

1. 所有值非零即正，且正元素的值等于“非零值数量”的倒数；
2. 不同的指示彼此独立，即所有指示向量的非零值位置不能重合。

这意味着解空间不是连续的，而是**离散**的；
为了求解这些指示向量，需要进行每个节点取舍的遍历。
这显然是一个NP-Hard问题。
因此，我们需要求它的近似解。

### 谱聚类的近似求解

近似求解的思路是一个两步走的思路

1. 放松对离散值的要求，
   利用特征值分解算法，求解连续空间中的替代指示向量；
2. 放松对损失函数的要求，
   利用K均值算法，将替代向量转化为严格的指示向量。

最后，我们将看到，
即使经过了两步放松，
我们找到的解也还是对全局最优解的合理估计。

#### 特征值分解

这里利用到一个简明的数学原理，即

> 实对称矩阵相似于实对角阵

$$\Lambda = U^T \cdot L \cdot U$$

其中，$\Lambda$为对角矩阵，且$U$为特征向量按列构成的对称矩阵

$$U^T \cdot U = I, U \cdot U^T = I $$

在$\Lambda$矩阵中，其对角线上的值为矩阵$L$的特征值，
我们可以将从小到大进行排列。
排列后，选择其中较小的$K$个特征值对应的特征向量$U_{1, 2, \dots, K}$，
就对应着使谱聚类损失函数最小的一组连续向量。

只可惜，它们尚不满足指示向量的离散要求。
但它们是我们在下一步，继续寻找指示向量的关键，
$$ U \rightarrow \hat{V} $$

因此，我们暂时称$U$为替代指示向量（矩阵），简称替代向量（矩阵）。
由于在这里无须严格区分具体的某一个指示向量和它们构成的矩阵，
因此暂时采用这种模糊的表达，而不致产生歧义。

#### K均值

下面我们将从连续的替代向量，求解得到离散的指示向量，
首先明确替代向量构成的矩阵的形式

$$U \in R^{N \times K}$$

其中，$N$代表节点数量，
$K$代表我们选取的较小的$K$个特征值的特征向量。

首先构造二次型，并考虑它的迹

$$tr(U \cdot I \cdot U^T) = \sum_{i=1}^{N} ||u_i||_2^2$$

其中，$tr(\cdot)$代表矩阵的迹，
$||u_i||_2^2 = \sum_{j=1}^{K}u_{ij}^2$是矩阵每个行向量的二范数。

从二范数的思路开展扩展，考虑K均值算法的损失函数

$$ argmin_{cut}\sum_i \sum_j ||x_{j} - c_i||_2^2 $$

其中，$c_i$代表第$i$类的聚类中心，
$x_{j}$代表第$j$个节点。

虽然看上去有些架空，但我们稍微将二次型进行变形，
引入满足指示向量规则的新向量组$H_{1, 2, \dots, K}$,
则K均值算法的损失函数，可以用如下方式进行表达

$$argmin_H tr((I - H_e)^T \cdot U \cdot I \cdot U^T \cdot (I - H_e))$$

其中，$H_e$代表按$H$矩阵扩展得到的矩阵扩展方法是将$H$的$K$个列，
按非零值的位置扩展成行。
去除上式中的不变项，我们得到

$$argmin_H tr(H^T \cdot U \cdot I \cdot U^T \cdot H)$$

这里，给$H$脱掉$e$的原因是这个扩展方法在失去$I-H_e$要求的条件下，
在计算上等价于每个独立的列只计算一次。

可以看到，从形式上，
指示向量构成的矩阵$H$与替代向量$U$具有一一对应的关系。
先给出推论，这些K均值算法找到的向量就是我们要找的指示向量。
下面将进行简要分析。

#### 误差估计

先从特征值分解的损失函数（第一次近似）开始，以它为核构造二次型。

$$ H^T \cdot U \cdot (U^T \cdot L \cdot U) \cdot U^T \cdot H$$

这里，我们相当于把其中的$\Lambda$
替换成了K均值算法损失函数（第二次近似）中的$I$。

由于$U$矩阵的正交性，它进一步可以简化为

$$H^T \cdot L \cdot H$$

由最小化K均值算法损失函数的计算原理可知，
$H$即是使谱聚类损失函数取得极小值的一个近似解。

误差主要考虑两点

1. 这种近似的误差出现在哪里呢？

   就在替换$I$和$\Lambda$对角阵的过程中。
   具体来说，当$\Lambda$矩阵与对角阵越相似，
   即在归一化之后，它主对角线上的值越相近，
   则近似效果越好。

2. 这种误差会造成什么影响呢？

   这个问题不是很大，因为一般来说，
   我们要处理的图只要不是过于奇葩，
   它的特征值在尾部的分布，一般都偏向于差异不大，
   也就是说，$Lambda$矩阵与$I$矩阵的差异一般不大。

所以这种近似方法，通常可以得到满意的结果。